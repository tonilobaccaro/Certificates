{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML321ENSkillsNetwork817-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Calculate Course Similarity using BoW Features**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity measurement between items is the foundation of many recommendation algorithms, especially for content-based recommendation algorithms. For example, if a new course is similar to user's enrolled courses, we could recommend that new similar course to the user. Or If user A is similar to user B, then we can recommend some of user B's courses to user A (the unseen courses) because user A and user B may have similar interests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous course, you learned many similarity measurements such as `consine`, `jaccard index`, or `euclidean distance`, and these methods need to work on either two vectors or two sets (sometimes even matrices or tensors). \n",
    "\n",
    "In previous labs, we extracted the BoW features from course textual content. Given the course BoW feature vectors, we can easily apply similarity measurement to calculate the course similarity as shown in the below figure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module_2/images/course_sim.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate the similarity between any two courses using BoW feature vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and setup lab environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's install and import required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3.tar.gz (23.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [21 lines of output]\n",
      "  + C:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python313\\python.exe C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-md93tdx1\\numpy_e9b37c4b599441eb999569b929da47c9\\vendored-meson\\meson\\meson.py setup C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-md93tdx1\\numpy_e9b37c4b599441eb999569b929da47c9 C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-md93tdx1\\numpy_e9b37c4b599441eb999569b929da47c9\\.mesonpy-m6997m5o -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-md93tdx1\\numpy_e9b37c4b599441eb999569b929da47c9\\.mesonpy-m6997m5o\\meson-python-native-file.ini\n",
      "  The Meson build system\n",
      "  Version: 1.2.99\n",
      "  Source dir: C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-md93tdx1\\numpy_e9b37c4b599441eb999569b929da47c9\n",
      "  Build dir: C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-md93tdx1\\numpy_e9b37c4b599441eb999569b929da47c9\\.mesonpy-m6997m5o\n",
      "  Build type: native build\n",
      "  Project name: NumPy\n",
      "  Project version: 1.26.4\n",
      "  WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "  \n",
      "  ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "  The following exception(s) were encountered:\n",
      "  Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  \n",
      "  A full log can be found at C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-md93tdx1\\numpy_e9b37c4b599441eb999569b929da47c9\\.mesonpy-m6997m5o\\meson-logs\\meson-log.txt\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "ERROR: Ignored the following yanked versions: 1.14.0rc1\n",
      "ERROR: Could not find a version that satisfies the requirement scipy==1.10 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.9.2, 1.9.3, 1.11.4, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.13.0rc1, 1.13.0, 1.13.1, 1.14.0rc2, 1.14.0, 1.14.1, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2)\n",
      "ERROR: No matching distribution found for scipy==1.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anton\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install scipy==1.10\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from gensim import corpora\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also set a random state\n",
    "rs = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the consine similarity between two example courses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have two simple example courses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course1 = \"machine learning for everyone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course2 = \"machine learning for beginners\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can quickly tokenize them using the split() method (or using `word_tokenize()` method provided in `nltk` as we did in the previous lab).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set(course1.split() + course2.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then generate BoW features (token counts) for these two courses (or using `tokens_dict.doc2bow()` method provided in `nltk`, similar to what we did in the previous lab).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sparse_bow(course):\n",
    "    \"\"\"\n",
    "    Generate a sparse bag-of-words (BoW) representation for a given course.\n",
    "\n",
    "    Parameters:\n",
    "    course (str): The input course text to generate the BoW representation for.\n",
    "\n",
    "    Returns:\n",
    "    list: A sparse BoW representation where each element corresponds to the presence (1) or absence (0)\n",
    "    of a word in the input course text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store the BoW vector\n",
    "    bow_vector = []\n",
    "\n",
    "    # Tokenize the course text by splitting it into words\n",
    "    words = course.split()\n",
    "\n",
    "    # Iterate through all unique words (tokens) in the course\n",
    "    for token in set(words):\n",
    "        # Check if the token is present in the course text\n",
    "        if token in words:\n",
    "            # If the token is present, append 1 to the BoW vector\n",
    "            bow_vector.append(1)\n",
    "        else:\n",
    "            # If the token is not present, append 0 to the BoW vector\n",
    "            bow_vector.append(0)\n",
    "\n",
    "    # Return the sparse BoW vector\n",
    "    return bow_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow1 = generate_sparse_bow(course1)\n",
    "bow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2 = generate_sparse_bow(course2)\n",
    "bow2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above cell outputs, we can see the two vectors are very similar. Only two dimensions are different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can quickly apply the cosine similarity measurement on the two vectors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = 1 - cosine(bow1, bow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The cosine similarity between course `{course1}` and course `{course2}` is {round(cos_sim, 2) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar courses to the course `Machine Learning with Python`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have learned how to calculate cosine similarity between two sample BoW feature vectors. Let's work on some real course BoW feature vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BoW features as Pandas dataframe\n",
    "bows_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/courses_bows.csv\"\n",
    "bows_df = pd.read_csv(bows_url)\n",
    "bows_df = bows_df[['doc_id', 'token', 'bow']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bows_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bows_df` dataframe contains the BoW features vectors for each course, in a vertical and dense format. It has three columns `doc_id` represents the course id, `token` represents the token value, and `bow` represents the BoW value (token count).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's load another course content dataset which contains the course title and description:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the course dataframe\n",
    "course_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/course_processed.csv\"\n",
    "course_df = pd.read_csv(course_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given course ID `ML0101ENv3`, let's find out its title and description:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_df[course_df['COURSE_ID'] == 'ML0101ENv3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it is a machine learning with Python course so we can expect any machine learning or Python related courses would be similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's print its associated BoW features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_course = bows_df[bows_df['doc_id'] == 'ML0101ENv3']\n",
    "ml_course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the BoW feature vector is in vertical format but normally feature vectors are in horizontal format. One way to transpose the feature vector from vertical to horizontal is to use the Pandas `pivot()` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_courseT = ml_course.pivot(index=['doc_id'], columns='token').reset_index(level=[0])\n",
    "ml_courseT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the BoWs of any two courses, which normally have a different set of tokens, we need to create a union token set and then transpose them. We have provided a method called `pivot_two_bows` as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_two_bows(basedoc, comparedoc):\n",
    "    \"\"\"\n",
    "    Pivot two bag-of-words (BoW) representations for comparison.\n",
    "\n",
    "    Parameters:\n",
    "    basedoc (DataFrame): DataFrame containing the bag-of-words representation for the base document.\n",
    "    comparedoc (DataFrame): DataFrame containing the bag-of-words representation for the document to compare.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with pivoted BoW representations for the base and compared documents,\n",
    "    facilitating direct comparison of word occurrences between the two documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create copies of the input DataFrames to avoid modifying the originals\n",
    "    base = basedoc.copy()\n",
    "    base['type'] = 'base'  # Add a 'type' column indicating base document\n",
    "    compare = comparedoc.copy()\n",
    "    compare['type'] = 'compare'  # Add a 'type' column indicating compared document\n",
    "\n",
    "    # Concatenate the two DataFrames vertically\n",
    "    join = pd.concat([base, compare])\n",
    "\n",
    "    # Pivot the concatenated DataFrame based on 'doc_id' and 'type', with words as columns\n",
    "    joinT = join.pivot(index=['doc_id', 'type'], columns='token').fillna(0).reset_index(level=[0, 1])\n",
    "\n",
    "    # Assign meaningful column names to the pivoted DataFrame\n",
    "    joinT.columns = ['doc_id', 'type'] + [t[1] for t in joinT.columns][2:]\n",
    "\n",
    "    # Return the pivoted DataFrame for comparison\n",
    "    return joinT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course1 = bows_df[bows_df['doc_id'] == 'ML0151EN']\n",
    "course2 = bows_df[bows_df['doc_id'] == 'ML0101ENv3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectors = pivot_two_bows(course1, course2)\n",
    "bow_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can use the cosine method to calculate their similarity:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = 1 - cosine(bow_vectors.iloc[0, 2:], bow_vectors.iloc[1, 2:])\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to perform a task of finding all courses similar to the course `Machine Learning with Python`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_df[course_df['COURSE_ID'] == 'ML0101ENv3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set a similarity threshold such as 0.5 to determine if two courses are similar enough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO: Find courses which are similar to course `Machine Learning with Python (ML0101ENv3)`, you also need to show the title and descriptions of those courses._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "courses = course_df['COURSE_ID'].unique()\n",
    "course_ML0101ENv3 = bows_df[bows_df['doc_id'] == 'ML0101ENv3']\n",
    "for course_id in np.delete(courses, np.where(courses == 'ML0101ENv3')):\n",
    "    course = bows_df[bows_df['doc_id'] == course_id]\n",
    "    bow_vectors = pivot_two_bows(course, course_ML0101ENv3)\n",
    "    similarity = 1 - cosine(bow_vectors.iloc[0, 2:], bow_vectors.iloc[1, 2:])\n",
    "    if similarity >= threshold:\n",
    "        print(course_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```toggle## Change Log\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```toggle|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "```\n",
    "```toggle|-|-|-|-|\n",
    "```\n",
    "```toggle|2021-10-25|1.0|Yan|Created the initial version|\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2021 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "prev_pub_hash": "fe64f6b0eef7c68ad33bd3860c93e16dead786ea94d3124912229fd158017944"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
